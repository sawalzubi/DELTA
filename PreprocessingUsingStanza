#################### IMPORTING REQUIRED PACKAGES ####################

import json
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import emoji
import stanza
from sklearn.feature_extraction.text import CountVectorizer
import csv

#################### DOWNLOADING NLTK LIBRARY ####################
nltk.download('stopwords')
nltk.download('punkt')


#################### REMOVING PUNCTUATIONS ####################
def spacing_punctuation(text):
    # preparing punctuation list
    arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ«»'''
    english_punctuations = string.punctuation
    punc_to_remove = ''.join(set(arabic_punctuations + english_punctuations))
    punc_to_keep = '+'
    punc_to_escape = '''[]-^'''
    for p in punc_to_keep: punc_to_remove = punc_to_remove.replace(p, '')
    for p in punc_to_escape: punc_to_remove = punc_to_remove.replace(p, '\\{}'.format(p))
    text = text.replace('\\n', ' ').replace('\n', ' ')
    text = text.replace('؛', '،')
    text = re.sub(r'\([^)]+\)', '', text)  # remove parentheses and everything in between
    text = re.sub(r'[a-zA-Z]', '', text)  # remove non-arabic characters
    # text = re.sub(r'\d+(\.\d+)?', ' رقم ', text)  # replace numbers by special token
    for p in punc_to_remove: text = text.replace(p, '')  # remove punctuations
    text = re.sub(r'(.)\1{2,}', r'\1', text)  # remove repeated chars
    text = re.sub(r'\s+', r' ', text)
    return text
#################### REMOVING URL ####################
def removingURL(text):
    s1 = re.sub('http://\S+|https://\S+', '', text)
    s2 = re.sub('http[s]?://\S+', '', text)
    s3 = re.sub(r"http\S+", "", text)
    if s1 in text:
        return s1
    elif s2 in text:
        return s2
    else:
        return s3
#################### REMOVING EMOJIS ####################
def remove_emojis(text):
    return emoji.replace_emoji(text, replace='')
#################### REMOVING STOP WORDS ####################
def removingStopWords(text):
    tokens = word_tokenize(text)
    arabic_stopwords = stopwords.words('arabic')
    tokens_no_stopwords = [t for t in tokens if t not in arabic_stopwords]
    clearedText = " ".join(tokens_no_stopwords)
    return clearedText
#################### FINDING WORDS LEMMA ####################
def lemma(text):
    nlp = stanza.Pipeline(lang='ar', processors='tokenize,mwt,pos,lemma')
    doc = nlp(text)
    print(*[f'word: {word.text + " "}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words],
          sep='\t\t')
#################### POS TAGS FINDER ####################
def taggerFinder(text):
    nlp = stanza.Pipeline(lang='ar', processors='tokenize,mwt,pos')
    doc = nlp(text)
    return [*[f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words]]
#################### NER LABELS FINDER ####################
def nerFinder(text):
    nlp = stanza.Pipeline(lang='ar', processors='tokenize,ner')
    doc = nlp(text)
    return [*[f'{token.text}_{token.ner}' for sent in doc.sentences for token in sent.tokens]]
#################### BAG OF WORDS GENERATOR ####################
def bagOfWords(text):
    vector = CountVectorizer()
    vector.fit(text)
    print(sorted(vector.vocabulary_))
    vector = vector.transform(text)
    print(vector.shape)
    print(vector.toarray())

#################### MAIN FUNCTION ####################

if __name__ == '__main__':
    listOfText = []
    listOfPOS = []
    listOfNER = []
    listOfBOW = []
    data = []
    header = ['text', 'rumorID', 'tweetID', 'POS', 'NER', 'Category']
    csvFile = open('/Users/ferasawaysheh/Documents/NLP contest /trainRumorWithoutPre.csv', mode='w',
                   encoding='UTF8', newline='')
    writer = csv.writer(csvFile)
    writer.writerow(header)
    #################### LOADING DATA FROM JSON FILE (TWEET TEXT & CATEGORY)  ####################
    with open('/Users/ferasawaysheh/Documents/NLP contest /train_rumors.json') as rumorFile:
        fileContent = json.load(rumorFile)
    for i in range(len(fileContent)):
        text = fileContent[i]['tweet_text']
        rumorID = fileContent[i]['rumor_id']
        tweetID = fileContent[i]['tweet_id']
        category = fileContent[i]['category']

        afterURL = removingURL(text)
        afterPunc = spacing_punctuation(afterURL)
        afterSW = removingStopWords(afterPunc)
        afterEji = remove_emojis(afterSW)
        listOfText.append(afterEji)
        listOfPOS = taggerFinder(afterEji)
        listOfNER = nerFinder(afterEji)
        data = [afterEji, rumorID, tweetID, listOfPOS, listOfNER, category]

        # lemmaz(text)
        # print(afterEji)
        # print(fileContent[i]['category'])
        # print('\n')
        # print('---------------------------------------------------------------')
        writer.writerow(data)

    rumorFile.close()
csvFile.close()
# bagOfWords(listOfText)
